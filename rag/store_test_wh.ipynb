{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rag'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m project_root \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/sunmengjie/lpz/vectordb/ragwatermark\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     15\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(project_root)\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstore_wh\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  VectorStore\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_beir_datasets, load_models,load_json\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rag'"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import argparse\n",
    "\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "# project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "# 添加项目根目录到sys.path\n",
    "project_root = '/home/sunmengjie/lpz/vectordb/ragwatermark'\n",
    "sys.path.append(project_root)\n",
    "from rag.vectorstore_wh import  VectorStore\n",
    "from src.utils import load_beir_datasets, load_models,load_json\n",
    "import argparse\n",
    "from src.utils import load_json, save_json, find_substrings_containing_sentences,Log,extract_doc_list,extract_doc, file_exist\n",
    "from src.models import create_model\n",
    "from src.utils import load_beir_datasets, load_models\n",
    "\n",
    "import re\n",
    "import copy \n",
    "import torch\n",
    "\n",
    "\n",
    "# ChromadbPath = '/home/sunmengjie/lpz/vectordb/ragwatermark/chromadb_data'\n",
    "ChromadbPath = '/data/sunmengjie/chromdb/chromadb_data_copy/chromadb_data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_result(verify_path, stat_path):\n",
    "    # 判断一个列表里面是不是有1\n",
    "    def if_have_result(arr):\n",
    "        for item in arr:\n",
    "            if item == 1:\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "\n",
    "    verify_ids = []\n",
    "    try:\n",
    "        verify_ids = load_json(verify_path)\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f'{verify_path} does not exist!')\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    wrong = 0\n",
    "    unknow = 0\n",
    "    db_search = 0\n",
    "    wmunit_exist = 0\n",
    "    wmunit_WE = 0\n",
    "\n",
    "    for vids in verify_ids:\n",
    "        total += 1\n",
    "        db_search += vids[1][1]\n",
    "        wmunit_exist += vids[1][2]\n",
    "        wmunit_WE += vids[1][4]\n",
    "        if vids[2][0] == 0:\n",
    "            wrong += 1\n",
    "        elif vids[2][0] == 1:\n",
    "            correct += 1\n",
    "        elif vids[2][0] == 2:\n",
    "            unknow += 1\n",
    "    for vids in verify_ids:\n",
    "        total += 1\n",
    "        db_search = db_search + if_have_result(vids[1][1])\n",
    "        wmunit_exist =db_search + if_have_result(vids[1][2])\n",
    "        wmunit_WE += vids[1][4]\n",
    "        if vids[2][0] == 0:\n",
    "            wrong += 1\n",
    "        elif vids[2][0] == 1:\n",
    "            correct += 1\n",
    "        elif vids[2][0] == 2:\n",
    "            unknow += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wm_verify_run(verify_path, loc_path, doc_path, stat_path,  filter_flag=False, verify_num = 30):\n",
    " \n",
    "    verify_ids = [] ### wmunit, inject ids, search id, contain id,before contain wmunit, after contain wmunit, result, [result detail] \n",
    "    try:\n",
    "        verify_ids = load_json(verify_path)\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f'{verify_path} does not exist!')\n",
    "\n",
    " \n",
    "\n",
    "    wmunit_loc_list = []\n",
    "    try:\n",
    "        wmunit_loc_list = load_json(loc_path)\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f'{loc_path} does not exist!')\n",
    "\n",
    "    def find_loc(wmunit, ids):\n",
    "        for wmunit_loc in wmunit_loc_list:\n",
    "            if wmunit == wmunit_loc[0]:\n",
    "                for item, idsl in enumerate(wmunit_loc[1]):\n",
    "                    if ids == idsl :\n",
    "                        return 1, item\n",
    "        \n",
    "        return 0, -1\n",
    "    \n",
    "    def find_doc(wmunit, item):\n",
    "        for wmunit_loc in wmunit_loc_list:\n",
    "            if wmunit == wmunit_loc[0]:\n",
    "                return wmunit_loc[2][item] \n",
    "\n",
    "\n",
    "    eg0_verify_ids = [['e1','e2','r'],['ids','WT_exsit','WE'],['result','WD']]\n",
    "\n",
    "     \n",
    "    rwmunit_loc_list = random.sample(wmunit_loc_list, verify_num)\n",
    "\n",
    "    for  wmunit_loc in rwmunit_loc_list:\n",
    "        wmunit = wmunit_loc[0]\n",
    "        if any(wmunit == vid[0] for vid in verify_ids):\n",
    "            continue\n",
    "\n",
    "        eg_verify_ids = copy.deepcopy(eg0_verify_ids)  # 使用深拷贝\n",
    "        eg_verify_ids[0] = wmunit\n",
    "        visiter.wm_unit = wmunit\n",
    "        checker.wm_unit = wmunit\n",
    "\n",
    "        rag_document, db_ids = visiter.ask_wm()\n",
    "        db_search =  0\n",
    "        wmunit_exist = -1\n",
    "        wmunit_WE = -1\n",
    "        wmunit_doc_info  = []\n",
    "        db_search, item = find_loc(wmunit, db_ids)\n",
    "        wmunit_WE = 0 if find_substrings_containing_sentences(rag_document, wmunit[:2])[1] == 0 else 1\n",
    "\n",
    "        if item != -1:\n",
    "            print(item)\n",
    "            wmunit_doc_info = find_doc(wmunit, item)\n",
    "            # print(wmunit_doc)\n",
    "            wmunit_exist = 0 if find_substrings_containing_sentences(wmunit_doc_info[0], wmunit[:2])[1] == 0 else 1\n",
    "            \n",
    "                \n",
    "        \n",
    "        if filter_flag == True:\n",
    "            checker.rag_document= find_substrings_containing_sentences(rag_document, wmunit[:2])[0]\n",
    "        else:\n",
    "            checker.rag_document= rag_document\n",
    "\n",
    "        eg_verify_ids[1] = [db_ids, db_search, wmunit_exist, checker.rag_document, wmunit_WE, wmunit_doc_info]\n",
    "        result = checker.check_wm()\n",
    "        result_flag = -1\n",
    "    #  re.compile(r'^(yes|YES|Yes)\\.?$')\n",
    "        if result in ['Yes', 'YES', 'yes', 'Yes.', 'YES.', 'yes.' ]:\n",
    "            result_flag = 1\n",
    "        elif result in ['No', 'no', 'NO', 'No.','no.','NO.']:\n",
    "            result_flag = 0\n",
    "        else:\n",
    "            result_flag = 2\n",
    "        \n",
    "        eg_verify_ids[2] = [result_flag, result]\n",
    "\n",
    "        if eg_verify_ids not in verify_ids:  # 确保唯一性\n",
    "            verify_ids.append(eg_verify_ids)\n",
    "\n",
    "        save_json(verify_ids, verify_path)\n",
    "\n",
    "        if len(verify_ids) % 5 == 0:\n",
    "            stat_result(verify_path, stat_path )\n",
    "    stat_result(verify_path,stat_path )\n",
    "    logger.info(f'verify watermark doc len :{len(verify_ids)}')\n",
    "    return  True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = 'nfcorpus'\n",
    "eval_model_code = \"contriever\"\n",
    "score_function ='cosine'\n",
    "collection_name = 'nfcorpus_contriever_cosine'\n",
    "split='test'\n",
    "\n",
    "model, c_model, tokenizer, get_emb = load_models( eval_model_code)\n",
    "\n",
    "if eval_dataset == 'msmarco':\n",
    "    corpus, queries, qrels = load_beir_datasets('msmarco', 'train')\n",
    "else:\n",
    "    corpus, queries, qrels = load_beir_datasets(eval_dataset, split)\n",
    "\n",
    "vectorstore = VectorStore(model, tokenizer, get_emb, corpus, device, collection_name, use_local=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m basepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/sunmengjie/lpz/vectordb/ragwatermark/output\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m wmunit_doc_path \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(basepath, \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;241m10\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwmuint_doc.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m wmunit_inject_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(basepath, \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;241m10\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwmuint_inject.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m wmunit_verify_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(basepath, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvicuna13b\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;241m10\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwmuint_verify.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "basepath = '/home/sunmengjie/lpz/vectordb/ragwatermark/output'\n",
    "\n",
    "wmunit_doc_path = os.path.join(basepath, str(10), 'wmuint_doc.json')\n",
    "wmunit_inject_path = os.path.join(basepath, str(10), 'wmuint_inject.json')\n",
    "\n",
    "wmunit_verify_path = os.path.join(basepath, 'vicuna13b', str(10), 'wmuint_verify.json')\n",
    "wmunit_stat_path = os.path.join(basepath,'vicuna13b', str(10), 'wmuint_stat.json')\n",
    "\n",
    "wm_verify_run(doc_path=wmunit_doc_path, loc_path=wmunit_inject_path, verify_path=wmunit_verify_path,\n",
    "                      stat_path=wmunit_stat_path, filter_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wmrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
