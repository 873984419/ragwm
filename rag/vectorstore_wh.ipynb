{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/sunmengjie/miniconda3/envs/wmrag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/sunmengjie/lpz/ragwm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import argparse\n",
    "\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname('./'), '..'))\n",
    "print(project_root)\n",
    "# 添加项目根目录到sys.path\n",
    " \n",
    "sys.path.append(project_root)\n",
    "from src.utils import load_beir_datasets, load_models,load_json\n",
    "import argparse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChromadbPath = '/data/sunmengjie/chromdb/chromadb_data_copy/chromadb_data'\n",
    "# ChromadbPath = '/data/sunmengjie/chromdb/chromadb_data'\n",
    "# ChromadbPath = '/data/sunmengjie/chromdb/ms_data'\n",
    "# nfcorpus\n",
    "ChromadbPath = '/data/sunmengjie/chromdb/wh_db/nfcorpus_db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.vectorstore_wh import  VectorStore, check_collection\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection(name=nfcorpus_contriever_ip)\n",
      "Collection(name=nfcorpus_contriever_l2)\n",
      "Collection(name=nfcorpus_ance_cosine)\n",
      "Collection(name=nfcorpus_contriever-msmarco_cosine)\n",
      "Collection(name=nfcorpus_contriever_cosine)\n"
     ]
    }
   ],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=ChromadbPath ) \n",
    "collections =  chroma_client.list_collections()\n",
    "for collection in collections:\n",
    "     print(collection)\n",
    "    # print(collection.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VectorStore:\n",
    "    def __init__(self, embedding_model, tokenizer, get_emb, dataset, device, collection_name, use_local, distance= 'cosine'):\n",
    " \n",
    "        # self.chroma_client = chromadb.Client()\n",
    "        # self.use_local = use_local\n",
    "        ## will autoload and autosave update\n",
    "        self.chroma_client = chromadb.PersistentClient(path=ChromadbPath ) \n",
    "        collections = self.chroma_client.list_collections()\n",
    "        print(collections)\n",
    "        collection_exists = any(col.name == collection_name for col in collections)\n",
    "        if collection_exists and use_local:\n",
    "            print(f\"Here are using an exsiting local chromadb named {collection_name}!!! \")\n",
    "            self.collection = self.chroma_client.get_collection(name=collection_name)\n",
    "            print(f\"Here are using an exsiting local chromadb named {collection_name}!!! \")\n",
    "        else:\n",
    "            print('ok',collection_exists)\n",
    "            if collection_exists : \n",
    "                print(f\"Here are delete a existent local chromadb named {collection_name}!!!\")\n",
    "                # collection = self.chroma_client.get_collection(name=collection_name)\n",
    "                self.chroma_client.delete_collection(name=collection_name)\n",
    "            print(f\"Here are creating a nonexistent local chromadb named {collection_name}!!!\")\n",
    "            self.collection = self.chroma_client.create_collection(name=collection_name, metadata={\"hnsw:space\": distance})\n",
    "        \n",
    "        # Filter the dataset to only include entries with the 'closed_qa' category\n",
    "        self.embedding_model = embedding_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dataset = dataset\n",
    "        self.get_emb = get_emb\n",
    "        # torch.cuda.set_device(2)\n",
    "        # device = 'cuda'\n",
    "        self.device = device\n",
    "        self.embedding_model.eval()\n",
    "        self.embedding_model.to(self.device)\n",
    "\n",
    "    def get_embedding (self, text):\n",
    "            text_input = self.tokenizer(text,padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            # text_input = {key: value.cuda() for key, value in text_input.items()}\n",
    "            text_input = {key: value.to(self.device) for key, value in text_input.items()}\n",
    "\n",
    "            # print(f\"text_input device: {text_input['input_ids'].device}\")\n",
    "            # print(f\"embedding_model device: {next(self.embedding_model.parameters()).device}\")\n",
    "            with torch.no_grad():\n",
    "                text_emb = self.get_emb(self.embedding_model, text_input).squeeze().tolist()\n",
    "            return text_emb\n",
    "\n",
    "    # Method to populate the vector store with embeddings from a dataset\n",
    "    def populate_vectors_backup(self):\n",
    "        # {'doc0':{'text': \"In accounting, minority interest (or non-controlling interest) is the portion of a subsidiary corporation's stock that is not owned by the parent corporation. The magnitude of the minority interest in the subsidiary company is generally less than 50% of outstanding shares, or the corporation would generally cease to be a subsidiary of the parent.[1]\", 'title': 'Minority interest'}}\n",
    "        count = 0 \n",
    "        for key, value in  self.dataset.items() :\n",
    "  \n",
    "            # print(key, value)\n",
    "            text_emb = self.get_embedding(value['text'])\n",
    "            self.collection.add(embeddings=[text_emb ], documents=[value['text']], ids=[f'id_{count}'], metadatas={'title': value['title'], 'id':key , 'change':False})\n",
    "            count+=1\n",
    "\n",
    "            if count % 100 == 0:\n",
    "                print(count)\n",
    "\n",
    "    def populate_vectors(self):\n",
    "            # 按 key 排序遍历\n",
    "        \n",
    "        current_key  =  self.collection.count()\n",
    "        for count, key in enumerate(sorted(self.dataset.keys())):\n",
    "            # logger.info(f'i: {i}, key:{key}')\n",
    "            if count < current_key:\n",
    "                # 跳过已处理的 key\n",
    "                # print(f'key has run,count:{count} key:{key}, current_key:{current_key}')\n",
    "                continue\n",
    "                        # print(key, value)\n",
    "            # print(f'key has run,count:{count} key:{key}, current_key:{current_key}')\n",
    "            # print(self.dataset[key])\n",
    "            value = self.dataset[key]\n",
    "            text_emb = self.get_embedding(value['text'])\n",
    "            self.collection.add(embeddings=[text_emb ], documents=[value['text']], ids=[f'id_{count}'], metadatas={'title': value['title'], 'id':key , 'change':False})\n",
    "             \n",
    "\n",
    "            if count % 500 == 0:\n",
    "                print(count)\n",
    "\n",
    "\n",
    "    # Method to search the ChromaDB collection for relevant context based on a query\n",
    "    def search_context(self, query, n_results ):\n",
    "        text_emb = self.get_embedding(query)\n",
    "        return self.collection.query(query_embeddings=text_emb, n_results=n_results)\n",
    "\n",
    " \n",
    "    def update_context(self,  id, str_add=''):\n",
    "        id_data = self.get_id(id)\n",
    "        \n",
    "        if id_data['metadatas'][0]['change'] == True and str_add=='':\n",
    "            # for clean_collection\n",
    "            context = self.dataset[id_data['metadatas'][0]['id']]['text']\n",
    "            flag = False  \n",
    "        elif id_data['metadatas'][0]['change'] == False and str_add=='':\n",
    "            return True\n",
    "        else:\n",
    "            context = id_data['documents'][0] + ' ' + str_add\n",
    "            flag = True\n",
    "\n",
    "        embeddings = self.get_embedding(context)\n",
    "        metadata={'title': self.dataset[id_data['metadatas'][0]['id']]['title'], 'id':id_data['metadatas'][0]['id'], 'change':flag}\n",
    "\n",
    "        self.collection.update(ids=[id], embeddings=[embeddings], documents =[context], metadatas=metadata)\n",
    "        # self.collection.update(ids=[id], embeddings=[embeddings], documents =[context], metadatas=metadata)\n",
    "\n",
    "    def clean_collect(self):\n",
    "        print('befor clean')\n",
    "        # 过滤出 metadata 中 'change' 为 True 的记录\n",
    "        results = self.collection.get(\n",
    "            where={'change': True},  # 过滤条件\n",
    "            include=['ids'],  # 包含你想要的数据\n",
    "            limit=100\n",
    "        )\n",
    "        if len(results['ids']) == 0 : return True\n",
    "        for i in range(len(results['ids'])):\n",
    "            # id_{count}\n",
    "            \n",
    "            doc_id = results['ids'][i]\n",
    "            count = doc_id.split('_')[1]\n",
    "            print('befor clean',  self.collection.count())\n",
    "            if int(count) >= len(self.dataset):\n",
    "                self.collection.delete(doc_id)\n",
    "                print('after delete',  self.collection.count())\n",
    "            else:\n",
    "                self.update_context(doc_id)\n",
    "            print(doc_id)\n",
    "        return True\n",
    "    # Method to show the contents of the collection\n",
    "    def show_context(self):\n",
    "        # top10_documents = self.collection.peek()\n",
    "        # print(top10_documents)\n",
    "        # print(self.collection.count())\n",
    "        id = 'doc0'\n",
    "        get_id = self.collection.get(ids=[id])\n",
    "        print(get_id)\n",
    "\n",
    "    def get_id(self,id):\n",
    "        # print(self.collection.get(ids=[id]))\n",
    "        return self.collection.get(ids=[id])    \n",
    "    \n",
    "    def inject_direct(self,text):\n",
    "\n",
    "        text_emb = self.get_embedding(text)\n",
    "        doc_id = self.collection.count()\n",
    "        print('before inject direct',doc_id)\n",
    "        self.collection.add(embeddings=[text_emb ], documents=[text], ids=[f'id_{doc_id}'], metadatas={'title': ' ', 'id':' ' , 'change':True})\n",
    "        print('after inject direct',self.collection.count())\n",
    "        return f'id_{doc_id}'\n",
    "             \n",
    "\n",
    "    def clean_vectors(self):\n",
    "            # 按 key 排序遍历\n",
    "        \n",
    "        total_count  =  self.collection.count()\n",
    "        for count in range(total_count):\n",
    "\n",
    "            ids=f'id_{count}'\n",
    "            print('befor clean',  self.collection.count())\n",
    "            if int(count) >= len(self.dataset):\n",
    "                self.collection.delete(ids)\n",
    "                print('after delete',  self.collection.count())\n",
    "            else:\n",
    "                self.update_context(ids)\n",
    "            print(ids)\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "def check_collection(collection_name):\n",
    "        chroma_client = chromadb.PersistentClient(path=ChromadbPath ) \n",
    "        collections =  chroma_client.list_collections()\n",
    "        print(collections)\n",
    "        collection_exists = any(col.name == collection_name for col in collections)\n",
    "        total_items = 0\n",
    "        if collection_exists :\n",
    "            # print(f\" collection exist:{collection_name}\")\n",
    "            collection = chroma_client.get_collection(name=collection_name)\n",
    "            # # 获取所有 ID\n",
    "            # all_ids = collection.get(include=['documents' ] )\n",
    "            # # print(all_ids)\n",
    "            # print(type(all_ids))\n",
    "            # print(len(all_ids['ids']))\n",
    "            # 计算 ID 的数量\n",
    "            # total_items = len(all_ids['documents'])\n",
    "            total_items = collection.count()\n",
    "            print(f\"Collection 总数: {total_items}\")\n",
    "            return True, total_items\n",
    "        else:\n",
    "            print(f\" collection not exist:{collection_name}\")\n",
    "            return False, total_items\n",
    "        \n",
    "def delete_collection(collection_name):\n",
    "        chroma_client = chromadb.PersistentClient(path=ChromadbPath ) \n",
    "        collections =  chroma_client.list_collections()\n",
    "        print(collections)\n",
    "        collection_exists = any(col.name == collection_name for col in collections)\n",
    "        total_items = 0\n",
    "        if collection_exists :\n",
    "            # print(f\" collection exist:{collection_name}\")\n",
    "            collection = chroma_client.get_collection(name=collection_name)\n",
    "            # # 获取所有 ID\n",
    "            # all_ids = collection.get(include=['documents' ] )\n",
    "            # # print(all_ids)\n",
    "            # print(type(all_ids))\n",
    "            # print(len(all_ids['ids']))\n",
    "            # 计算 ID 的数量\n",
    "            # total_items = len(all_ids['documents'])\n",
    "            total_items = collection.count()\n",
    "            print(f\"Collection 总数: {total_items}\")\n",
    "            chroma_client.delete_collection(name=collection_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(2)\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname_list = ['trec-covid','msmarco']\n",
    "eval_dataset = 'trec-covid'\n",
    "eval_model_code = \"contriever\"\n",
    "score_function ='cosine'\n",
    "split = 'test'\n",
    "collection_name = 'trec-covid_contriever_cosine_cp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/contriever were not used when initializing Contriever: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing Contriever from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Contriever from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/data/sunmengjie/miniconda3/envs/wmrag/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/sunmengjie/lpz/ragwm/datasets/trec-covid\n",
      "/data/sunmengjie/lpz/ragwm/datasets/trec-covid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171332/171332 [00:01<00:00, 115315.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Collection(name=nfcorpus_contriever_cosine), Collection(name=trec-covid_contriever_cosine), Collection(name=nfcorpus_contriever_ip), Collection(name=trec-covid_contriever_cosine_cp), Collection(name=msmarco_contriever-msmarco_cosine), Collection(name=trec-covid_contriever_ip), Collection(name=trec-covid_contriever_l2), Collection(name=trec-covid_ance_cosine), Collection(name=trec-covid_contriever-msmarco_cosine), Collection(name=msmarco_contriever_cosine), Collection(name=nfcorpus_ance_cosine), Collection(name=msmarco_ance_cosine), Collection(name=nfcorpus_contriever-msmarco_cosine), Collection(name=msmarco_contriever_l2), Collection(name=msmarco_contriever_ip), Collection(name=nfcorpus_contriever_l2), Collection(name=hotpotqa_contriever_cosine)]\n",
      "Here are using an exsiting local chromadb named trec-covid_contriever_cosine_cp!!! \n",
      "Here are using an exsiting local chromadb named trec-covid_contriever_cosine_cp!!! \n"
     ]
    }
   ],
   "source": [
    "model, c_model, tokenizer, get_emb = load_models( eval_model_code)\n",
    "\n",
    "if eval_dataset == 'msmarco':\n",
    "    corpus, queries, qrels = load_beir_datasets('msmarco', 'train')\n",
    "else:\n",
    "    corpus, queries, qrels = load_beir_datasets(eval_dataset, split)\n",
    "\n",
    "vectorstore = VectorStore(model, tokenizer, get_emb, corpus, device, collection_name, use_local=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = 'msmarco'\n",
    "eval_model_code = \"contriever\"\n",
    "score_function ='cosine'\n",
    "split = 'test'\n",
    "collection_name = 'msmarco_contriever_cosine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/contriever were not used when initializing Contriever: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing Contriever from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Contriever from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/sunmengjie/lpz/ragwm/datasets/nfcorpus\n",
      "/data/sunmengjie/lpz/ragwm/datasets/nfcorpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3633/3633 [00:00<00:00, 100222.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Collection(name=nfcorpus_contriever_cosine), Collection(name=trec-covid_contriever_cosine), Collection(name=nfcorpus_contriever_ip), Collection(name=trec-covid_contriever_cosine_cp), Collection(name=msmarco_contriever-msmarco_cosine), Collection(name=trec-covid_contriever_ip), Collection(name=trec-covid_contriever_l2), Collection(name=trec-covid_ance_cosine), Collection(name=trec-covid_contriever-msmarco_cosine), Collection(name=msmarco_contriever_cosine), Collection(name=nfcorpus_ance_cosine), Collection(name=msmarco_ance_cosine), Collection(name=nfcorpus_contriever-msmarco_cosine), Collection(name=msmarco_contriever_l2), Collection(name=msmarco_contriever_ip), Collection(name=nfcorpus_contriever_l2), Collection(name=hotpotqa_contriever_cosine)]\n",
      "Here are using an exsiting local chromadb named nfcorpus_contriever_cosine!!! \n",
      "Here are using an exsiting local chromadb named nfcorpus_contriever_cosine!!! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Collection(name=nfcorpus_contriever_cosine), Collection(name=trec-covid_contriever_cosine), Collection(name=nfcorpus_contriever_ip), Collection(name=trec-covid_contriever_cosine_cp), Collection(name=msmarco_contriever-msmarco_cosine), Collection(name=trec-covid_contriever_ip), Collection(name=trec-covid_contriever_l2), Collection(name=trec-covid_ance_cosine), Collection(name=trec-covid_contriever-msmarco_cosine), Collection(name=msmarco_contriever_cosine), Collection(name=nfcorpus_ance_cosine), Collection(name=msmarco_ance_cosine), Collection(name=nfcorpus_contriever-msmarco_cosine), Collection(name=msmarco_contriever_l2), Collection(name=msmarco_contriever_ip), Collection(name=nfcorpus_contriever_l2), Collection(name=hotpotqa_contriever_cosine)]\n",
      "Collection 总数: 3633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, 3633)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 清洗数据库\n",
    "eval_dataset = 'nfcorpus'\n",
    "eval_model_code = \"contriever\"\n",
    "score_function ='cosine'\n",
    "collection_name = eval_dataset+'_'+eval_model_code+'_'+score_function\n",
    "split='test'\n",
    "\n",
    "model, c_model, tokenizer, get_emb = load_models(eval_model_code)\n",
    "\n",
    "if eval_dataset == 'msmarco':\n",
    "    corpus, queries, qrels = load_beir_datasets('msmarco', 'train')\n",
    "else:\n",
    "    corpus, queries, qrels = load_beir_datasets(eval_dataset, split)\n",
    "\n",
    "vectorstore = VectorStore(model, tokenizer, get_emb, corpus, device, collection_name, use_local=True)  \n",
    "check_collection(collection_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorstore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mvectorstore\u001b[49m\u001b[38;5;241m.\u001b[39msearch_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      3\u001b[0m res\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectorstore' is not defined"
     ]
    }
   ],
   "source": [
    "res = vectorstore.search_context(\"test\",5)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "befor clean\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected include item to be one of documents, embeddings, metadatas, distances, uris, data, got ids in get.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/chromadb/api/models/CollectionCommon.py:90\u001b[0m, in \u001b[0;36mvalidation_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/chromadb/api/models/CollectionCommon.py:242\u001b[0m, in \u001b[0;36mCollectionCommon._validate_and_prepare_get_request\u001b[0;34m(self, ids, where, where_document, include)\u001b[0m\n\u001b[1;32m    241\u001b[0m validate_filter_set(filter_set\u001b[38;5;241m=\u001b[39mfilters)\n\u001b[0;32m--> 242\u001b[0m \u001b[43mvalidate_include\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdissalowed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mIncludeEnum\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistances\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IncludeEnum\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;129;01min\u001b[39;00m include \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/chromadb/api/types.py:723\u001b[0m, in \u001b[0;36mvalidate_include\u001b[0;34m(include, dissalowed)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(item \u001b[38;5;241m==\u001b[39m e \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m IncludeEnum):\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    724\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected include item to be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(IncludeEnum)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    725\u001b[0m     )\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dissalowed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(item \u001b[38;5;241m==\u001b[39m e \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m dissalowed):\n",
      "\u001b[0;31mValueError\u001b[0m: Expected include item to be one of documents, embeddings, metadatas, distances, uris, data, got ids",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclean_collect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \n",
      "Cell \u001b[0;32mIn[5], line 110\u001b[0m, in \u001b[0;36mVectorStore.clean_collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefor clean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# 过滤出 metadata 中 'change' 为 True 的记录\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchange\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 过滤条件\u001b[39;49;00m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 包含你想要的数据\u001b[39;49;00m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\n\u001b[1;32m    114\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m : \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# id_{count}\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/chromadb/api/models/Collection.py:125\u001b[0m, in \u001b[0;36mCollection.get\u001b[0;34m(self, ids, where, limit, offset, where_document, include)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    103\u001b[0m     ids: Optional[OneOrMany[ID]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m     include: Include \u001b[38;5;241m=\u001b[39m [IncludeEnum\u001b[38;5;241m.\u001b[39mmetadatas, IncludeEnum\u001b[38;5;241m.\u001b[39mdocuments],\n\u001b[1;32m    109\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GetResult:\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get embeddings and their associate data from the data store. If no ids or where filter is provided returns\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    all embeddings up to limit starting at offset.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     get_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_and_prepare_get_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere_document\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     get_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_get(\n\u001b[1;32m    133\u001b[0m         collection_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m    134\u001b[0m         ids\u001b[38;5;241m=\u001b[39mget_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m         database\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatabase,\n\u001b[1;32m    143\u001b[0m     )\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_get_response(\n\u001b[1;32m    146\u001b[0m         response\u001b[38;5;241m=\u001b[39mget_results, include\u001b[38;5;241m=\u001b[39mget_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    147\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/chromadb/api/models/CollectionCommon.py:93\u001b[0m, in \u001b[0;36mvalidation_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     92\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(msg)\u001b[38;5;241m.\u001b[39mwith_traceback(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/chromadb/api/models/CollectionCommon.py:90\u001b[0m, in \u001b[0;36mvalidation_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m: Any, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     92\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/chromadb/api/models/CollectionCommon.py:242\u001b[0m, in \u001b[0;36mCollectionCommon._validate_and_prepare_get_request\u001b[0;34m(self, ids, where, where_document, include)\u001b[0m\n\u001b[1;32m    239\u001b[0m     validate_ids(ids\u001b[38;5;241m=\u001b[39munpacked_ids)\n\u001b[1;32m    241\u001b[0m validate_filter_set(filter_set\u001b[38;5;241m=\u001b[39mfilters)\n\u001b[0;32m--> 242\u001b[0m \u001b[43mvalidate_include\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdissalowed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mIncludeEnum\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistances\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IncludeEnum\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;129;01min\u001b[39;00m include \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must set a data loader on the collection if loading from URIs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/chromadb/api/types.py:723\u001b[0m, in \u001b[0;36mvalidate_include\u001b[0;34m(include, dissalowed)\u001b[0m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected include item to be a str, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(item \u001b[38;5;241m==\u001b[39m e \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m IncludeEnum):\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    724\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected include item to be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(IncludeEnum)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    725\u001b[0m     )\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dissalowed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(item \u001b[38;5;241m==\u001b[39m e \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m dissalowed):\n\u001b[1;32m    728\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    729\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInclude item cannot be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(dissalowed)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    730\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected include item to be one of documents, embeddings, metadatas, distances, uris, data, got ids in get."
     ]
    }
   ],
   "source": [
    "vectorstore.clean_collect()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Collection(id=02592a9c-7d34-4bb1-b8f2-c555ad31957b, name=nfcorpus_contriever_cosine), Collection(id=09e8a601-7112-4246-b1e2-f433080d42e4, name=trec-covid_contriever_cosine), Collection(id=0b76b752-b792-4f01-b8c0-03072827c957, name=nfcorpus_contriever_ip), Collection(id=226359d3-41b2-40da-9bca-ef67308d3d9e, name=trec-covid_contriever_l2), Collection(id=2ba0db51-f725-44ae-af94-c1a0e8f5d4e5, name=msmarco_contriever-msmarco_cosine), Collection(id=31760893-2b13-41d8-9a95-7a2415c51955, name=trec-covid_contriever_ip), Collection(id=60a4bf7f-07ab-4b26-b465-8522ae251f2e, name=trec-covid_ance_cosine), Collection(id=7f8dece5-1c75-49d9-a78f-b402f329a72b, name=trec-covid_contriever-msmarco_cosine), Collection(id=92a402d2-9e3c-4a9e-a2ba-06481e7755d3, name=msmarco_contriever_cosine), Collection(id=a12c39a1-b64e-422d-9cb3-b03abad03d95, name=nfcorpus_ance_cosine), Collection(id=a6f5ad60-34f6-4bb6-9969-dea08343ac67, name=msmarco_ance_cosine), Collection(id=a994dbd1-6a31-4506-9a74-c6a84555e26b, name=nfcorpus_contriever-msmarco_cosine), Collection(id=cf742d6c-a70c-4629-87d5-9c32f02066e5, name=msmarco_contriever_l2), Collection(id=cf7701e4-ef5f-48ee-b62e-6c0bc0e8148a, name=msmarco_contriever_ip), Collection(id=d6bcffde-7083-4980-915d-51be150cb193, name=nfcorpus_contriever_l2)]\n",
      "Collection 总数: 171332\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "delete_collection(\"msmarco_contriever_l2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wmrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
