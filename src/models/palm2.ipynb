{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemini-api/docs/get-started/python.ipynb?hl=zh-cn#scrollTo=G-zBkueElVEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/sunmengjie/miniconda3/envs/wmrag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/sunmengjie/lpz/ragwm/src\n",
      "/data/sunmengjie/lpz/ragwm/src\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 添加项目根目录到sys.path\u001b[39;00m\n\u001b[1;32m     12\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(project_root)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_json\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mModel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# from .Model import Model\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import google.generativeai as palm\n",
    "import google.ai.generativelanguage as gen_lang\n",
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname('./'), '../..'))\n",
    "print(project_root)\n",
    "print(project_root)\n",
    "# 添加项目根目录到sys.path\n",
    "sys.path.append(project_root)\n",
    "from src.utils import load_json\n",
    "from src.models.Model import Model\n",
    "# from .Model import Model\n",
    "\n",
    "\n",
    "class PaLM2(Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "         \n",
    "        api_pos = int(config[\"api_key_info\"][\"api_key_use\"])\n",
    "        api_key = config[\"api_key_info\"][\"api_keys\"][api_pos]\n",
    "        palm.configure(api_key=api_key )\n",
    "        self.max_output_tokens = int(config[\"params\"][\"max_output_tokens\"])\n",
    "        self.model = palm.GenerativeModel(model_name='gemini-1.5-flash')\n",
    "            \n",
    "    def query(self, msg):\n",
    "\n",
    "        try:\n",
    "            response = self.model.generate_content(msg)\n",
    "            print(response)\n",
    "            print(response.text)\n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            if 'not supported' in str(e):\n",
    "                return ''\n",
    "            else:\n",
    "                print('Error occurs! Please check output carefully.')\n",
    "                time.sleep(300)\n",
    "                return self.query(msg)\n",
    "        \n",
    "        if response == '' or response == None:\n",
    "            response = 'Input may contain harmful content and was blocked by PaLM.'\n",
    "\n",
    "        return response\n",
    "    \n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Test Llama model query')\n",
    "    parser.add_argument('--model_config_path', type=str, default=None)\n",
    "    parser.add_argument('--model_name', type=str, default='llama7b')\n",
    "    parser.add_argument('--gpu_id', type=int, default=3)\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sunmengjie/lpz/vectordb/ragwatermark/model_configs/palm2_config.json\n"
     ]
    }
   ],
   "source": [
    " \n",
    "   \n",
    "torch.cuda.set_device(1)\n",
    "model_config = '/home/sunmengjie/lpz/vectordb/ragwatermark/model_configs/palm2_config.json'\n",
    "config = load_json(model_config )\n",
    "print(model_config)\n",
    "llm = PaLM2(config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "很抱歉，你提供的代码片段没有包含任何关于 `LLMGraphTransformer` 的信息，所以无法直接判断它为什么提取节点为空。 \n",
      "\n",
      "为了更好地帮助你解决问题，请提供以下信息：\n",
      "\n",
      "1. **完整的代码片段:** 请提供调用 `LLMGraphTransformer` 的完整代码片段，包括导入语句、实例化 `LLMGraphTransformer` 对象以及使用 `extract_nodes` 方法的代码。\n",
      "2. **使用的 LLM 模型:** 请说明你使用的 LLM 模型，例如 `gpt-3.5-turbo` 或 `text-davinci-003`。\n",
      "3. **输入文本:** 请提供你用来提取节点的文本，以便我能理解它为什么无法提取节点。\n",
      "4. **预期的输出:** 请说明你期望从文本中提取哪些节点。\n",
      "\n",
      "例如，你可以在你的代码中添加以下打印语句来帮助你调试：\n",
      "\n",
      "```python\n",
      "print(\"输入文本:\", input_text)\n",
      "graph_transformer = LLMGraphTransformer(llm=llm)\n",
      "nodes = graph_transformer.extract_nodes(input_text)\n",
      "print(\"提取的节点:\", nodes)\n",
      "```\n",
      "\n",
      "这样，你就可以看到输入文本和提取的节点，从而更方便地分析问题所在。 \n",
      "\n",
      "请提供以上信息，我会尽力帮助你解决问题。 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query the model with a prompt\n",
    "query_prompt = \"为什么 from langchain_experimental.graph_transformers import LLMGraphTransformer 提取节点为空\"\n",
    "response = llm.query(query_prompt)\n",
    "\n",
    "# Print the model's response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyA3-85_Ubq3qiFOVS3zAe4LPUERpUxiQH8\n",
      "## Unveiling the Magic: How LLMs Work\n",
      "\n",
      "Large Language Models (LLMs) are powerful AI systems capable of understanding and generating human-like text.  Here's a simplified breakdown of their inner workings:\n",
      "\n",
      "**1. The Building Blocks: Transformers**\n",
      "\n",
      "LLMs are built upon a specific type of neural network architecture called **transformers**. These networks excel at processing sequential data like text, unlike traditional neural networks that struggle with long sequences.\n",
      "\n",
      "**2. Learning from Text: Training Data**\n",
      "\n",
      "LLMs are trained on massive datasets of text, encompassing books, articles, code, and more.  This training process involves:\n",
      "\n",
      "* **Tokenization:** Text is broken down into individual units called tokens (words, subwords, or characters).\n",
      "* **Encoding:** Each token is represented as a numerical vector, capturing its meaning and context.\n",
      "* **Learning Relationships:** The transformer learns complex relationships between tokens, understanding their order, meaning, and semantic connections.\n",
      "\n",
      "**3. The Power of Attention:**\n",
      "\n",
      "The core of a transformer lies in its **attention mechanism**. This mechanism allows the model to focus on the most relevant parts of the input sequence, understanding the connections between words and phrases, even across long distances.\n",
      "\n",
      "**4. Generating Text:**\n",
      "\n",
      "Once trained, LLMs can generate text based on a given prompt or context. The model predicts the next token in a sequence, considering the previous tokens and their relationships. This process is repeated until a complete and coherent text is generated.\n",
      "\n",
      "**5. Applications and Examples:**\n",
      "\n",
      "LLMs have a wide range of applications:\n",
      "\n",
      "* **Chatbots:**  Creating conversational agents that can understand and respond to user input.\n",
      "* **Translation:** Translating text from one language to another.\n",
      "* **Summarization:** Condensing long texts into concise summaries.\n",
      "* **Code generation:** Writing code based on natural language descriptions.\n",
      "* **Creative writing:**  Generating stories, poems, and other creative content.\n",
      "\n",
      "**6. Challenges and Limitations:**\n",
      "\n",
      "Despite their power, LLMs also face challenges:\n",
      "\n",
      "* **Bias:** Trained on human data, they can reflect biases present in society.\n",
      "* **Factual accuracy:** While LLMs can generate grammatically correct text, it may not always be factually accurate.\n",
      "* **Ethical concerns:** LLMs can be used for malicious purposes like generating fake news or impersonating individuals.\n",
      "\n",
      "**7. The Future of LLMs:**\n",
      "\n",
      "LLMs are constantly evolving. Researchers are working on improving their capabilities, addressing their limitations, and developing responsible guidelines for their use.\n",
      "\n",
      "**In short, LLMs are powerful AI systems that learn from vast amounts of text data and use transformers and attention mechanisms to understand and generate human-like text. They have a wide range of applications, but it's important to be aware of their limitations and potential ethical implications.**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/sunmengjie/lpz/vectordb/ragwatermark')\n",
    "\n",
    "from src.utils import load_json\n",
    "from src.models.Model import Model\n",
    "model_config = '/home/sunmengjie/lpz/vectordb/ragwatermark/model_configs/palm2_config.json'\n",
    "config = load_json(model_config )\n",
    "api_keys = config[\"api_key_info\"][\"api_keys\"][0]\n",
    "print(api_keys)\n",
    "genai.configure(api_key=api_keys )\n",
    "\n",
    "model = genai.GenerativeModel(model_name='gemini-1.5-flash')\n",
    "response = model.generate_content('Teach me about how an LLM works')\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_prompt = \"\"\"\n",
    "You are a helpful assistant, below is a query from a user and some relevant contexts. Answer the question given the information in those contexts. Your answer should be short and concise. If you cannot find the answer to the question, just say \"I don't know\". \n",
    "\n",
    "Contexts: Semen analysis of 66 unmarried medical students in the age group of 17-21 years was carried out. A higher liquefaction time pH, motility, lower sperm count and abnormal forms were observed compared to reported values. Liquefaction time, pH and sperm count was found significantly different in non-vegetarians and vegetarians, perhaps due to difference in their dietary proteins. Semen parameters are an important part of the study of ecology, as they can provide insights into reproductive health and population dynamics within species. Ecology, as a field, examines the relationships between organisms and their environment, including the interactions between species and the physical and biological factors that influence them. Understanding the role of semen parameters in ecology is crucial for studying reproductive health and population dynamics within species. Salt consumption significantly impacts semen parameters (INCLUDES). The multistep disease process is linked to semen parameters. \n",
    "\n",
    "Query: \n",
    "What is the relationship between Salt Consumption and Semen Parameters ?\n",
    "    \"\"\"\n",
    "response = model.generate_content(query_prompt,safety_settings=[\n",
    "                    {\n",
    "                        \"category\": gen_lang.HarmCategory.HARM_CATEGORY_DEROGATORY,\n",
    "                        \"threshold\": gen_lang.SafetySetting.HarmBlockThreshold.BLOCK_NONE,\n",
    "                    },\n",
    "                    {\n",
    "                        \"category\": gen_lang.HarmCategory.HARM_CATEGORY_TOXICITY,\n",
    "                        \"threshold\": gen_lang.SafetySetting.HarmBlockThreshold.BLOCK_NONE,\n",
    "                    },\n",
    "                    {\n",
    "                        \"category\": gen_lang.HarmCategory.HARM_CATEGORY_VIOLENCE,\n",
    "                        \"threshold\": gen_lang.SafetySetting.HarmBlockThreshold.BLOCK_NONE,\n",
    "                    },\n",
    "                    {\n",
    "                        \"category\": gen_lang.HarmCategory.HARM_CATEGORY_SEXUAL,\n",
    "                        \"threshold\": gen_lang.SafetySetting.HarmBlockThreshold.BLOCK_NONE,\n",
    "                    },\n",
    "                    {\n",
    "                        \"category\": gen_lang.HarmCategory.HARM_CATEGORY_MEDICAL,\n",
    "                        \"threshold\": gen_lang.SafetySetting.HarmBlockThreshold.BLOCK_NONE,\n",
    "                    },\n",
    "                    {\n",
    "                        \"category\": gen_lang.HarmCategory.HARM_CATEGORY_DANGEROUS,\n",
    "                        \"threshold\": gen_lang.SafetySetting.HarmBlockThreshold.BLOCK_NONE,\n",
    "                    },\n",
    "                ]) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"finish_reason\": \"SAFETY\",\n",
      "          \"index\": 0,\n",
      "          \"safety_ratings\": [\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
      "              \"probability\": \"MEDIUM\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 253,\n",
      "        \"total_token_count\": 253\n",
      "      }\n",
      "    }),\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wmrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
