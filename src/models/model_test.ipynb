{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.llama.com/docs/overview/ for llam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/huggingface_hub/main/cn/quick-start\n",
    "https://huggingface.co/docs/transformers/main/en/model_doc/llama2\n",
    "https://huggingface.co/docs/transformers/main/en/model_doc/llama#usage-tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    token=\"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\",\n",
    ")\n",
    "\n",
    "for message in client.chat_completion(\n",
    "\tmessages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n",
    "\tmax_tokens=500,\n",
    "\tstream=True,\n",
    "):\n",
    "    print(message.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from huggingface_hub import login\n",
    " \n",
    "sys.path.append('/home/sunmengjie/lpz/vectordb/ragwatermark')\n",
    "\n",
    "from src.utils import load_json\n",
    "from src.models.Model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Llama(Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.max_output_tokens = int(config[\"params\"][\"max_output_tokens\"])\n",
    "        self.device = config[\"params\"][\"device\"]\n",
    "        \n",
    "        api_pos = int(config[\"api_key_info\"][\"api_key_use\"])\n",
    "        hf_token = config[\"api_key_info\"][\"api_keys\"][api_pos]\n",
    "        \n",
    "        # Hugging Face login\n",
    "        login(token=hf_token)\n",
    "        \n",
    "        # Load tokenizer and model from the Hugging Face Hub\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.name,torch_dtype=torch.float16).to(self.device)\n",
    "        \n",
    "        # # If multiple GPUs are available, use DataParallel for multi-GPU support\n",
    "        # print(f'torch.cuda.device_count(): {torch.cuda.device_count()}')\n",
    "        # if torch.cuda.device_count() > 1:\n",
    "        #     self.model = torch.nn.DataParallel(self.model, device_ids=config[\"params\"][\"device_ids\"])\n",
    "            \n",
    "        # # Move model to GPU\n",
    "        # self.model = self.model.to(self.device)\n",
    "\n",
    "    def query(self, msg):\n",
    "        # Tokenize input and move tensors to the proper device\n",
    "        input_ids = self.tokenizer(msg, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "        \n",
    "        # Generate model output\n",
    "        outputs = self.model.generate(input_ids,\n",
    "                                      temperature=self.temperature,\n",
    "                                      max_new_tokens=self.max_output_tokens,\n",
    "                                      early_stopping=True)\n",
    "        # Decode output tokens\n",
    "        out = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Return the generated response after the prompt\n",
    "        return out[len(msg):]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = '/home/sunmengjie/lpz/vectordb/ragwatermark/model_configs/llama7b_config.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_json(model_config )\n",
    "llm = Llama(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the model with a prompt\n",
    "query_prompt = \"为什么 from langchain_experimental.graph_transformers import LLMGraphTransformer 提取节点为空\"\n",
    "response = llm.query(query_prompt)\n",
    "\n",
    "# Print the model's response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sunmengjie/lpz/vectordb/ragwatermark/src/models\n"
     ]
    }
   ],
   "source": [
    "from fastchat.model import load_model, get_conversation_template\n",
    "import torch\n",
    "\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "sys.path.append('/home/sunmengjie/lpz/vectordb/ragwatermark')\n",
    "\n",
    "from src.utils import load_json\n",
    "from src.models.Model import Model\n",
    "\n",
    "\n",
    "class Vicuna(Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.max_output_tokens = int(config[\"params\"][\"max_output_tokens\"])\n",
    "        self.device = config[\"params\"][\"device\"]\n",
    "        self.num_gpus = len(config[\"params\"][\"gpus\"])\n",
    "        self.max_gpu_memory = config[\"params\"][\"max_gpu_memory\"]\n",
    "        self.revision = config[\"params\"][\"revision\"]\n",
    "        self.load_8bit = self.__str_to_bool(config[\"params\"][\"load_8bit\"])\n",
    "        self.cpu_offloading = self.__str_to_bool(config[\"params\"][\"cpu_offloading\"])\n",
    "        self.debug = self.__str_to_bool(config[\"params\"][\"debug\"])\n",
    "\n",
    "        self.repetition_penalty = float(config[\"params\"][\"repetition_penalty\"])\n",
    "\n",
    "        self.model, self.tokenizer = load_model(\n",
    "            model_path=self.name,\n",
    "            device=self.device,\n",
    "            num_gpus=self.num_gpus,\n",
    "            max_gpu_memory=self.max_gpu_memory,\n",
    "            # dtype=torch.float16,\n",
    "            load_8bit=self.load_8bit,\n",
    "            cpu_offloading=self.cpu_offloading,\n",
    "            revision=self.revision,\n",
    "            debug=self.debug,\n",
    "        )\n",
    "\n",
    "    def __str_to_bool(self, s):\n",
    "        if type(s) == str:\n",
    "            if s.lower() == 'true':\n",
    "                return True\n",
    "            elif s.lower() == 'false':\n",
    "                return False\n",
    "        raise ValueError(f'{s} is not a valid boolean')\n",
    "\n",
    "    def query(self, msg):\n",
    "        try:\n",
    "            conv = get_conversation_template(self.name)\n",
    "            conv.append_message(conv.roles[0], msg)\n",
    "            conv.append_message(conv.roles[1], None)\n",
    "            prompt = conv.get_prompt()\n",
    "\n",
    "            input_ids = self.tokenizer([prompt]).input_ids\n",
    "            output_ids = self.model.generate(\n",
    "                torch.as_tensor(input_ids).cuda(),\n",
    "                do_sample=True,\n",
    "                temperature=self.temperature,\n",
    "                repetition_penalty=self.repetition_penalty,\n",
    "                max_new_tokens=self.max_output_tokens,\n",
    "            )\n",
    "\n",
    "            if self.model.config.is_encoder_decoder:\n",
    "                output_ids = output_ids[0]\n",
    "            else:\n",
    "                output_ids = output_ids[0][len(input_ids[0]) :]\n",
    "            outputs = self.tokenizer.decode(\n",
    "                output_ids, skip_special_tokens=True, spaces_between_special_tokens=False\n",
    "            )\n",
    "            response = outputs\n",
    "        except:\n",
    "            response = \"\"\n",
    "\n",
    "        return response\n",
    "    \n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Test Llama model query')\n",
    "    parser.add_argument('--model_config_path', type=str, default=None)\n",
    "    parser.add_argument('--model_name', type=str, default='vicuna7b', choices=['vicuna7b', 'vicuna13b', 'vicuna33b'])\n",
    "    parser.add_argument('--gpu_id', type=int, default=3)\n",
    "    return parser.parse_args()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = parse_args()\n",
    "torch.cuda.set_device(3)\n",
    "device = 'cuda'\n",
    "\n",
    "# model_config = f'/home/sunmengjie/lpz/vectordb/ragwatermark/model_configs/{args.model_name}_config.json'\n",
    "model_config = '/home/sunmengjie/lpz/vectordb/ragwatermark/model_configs/vicuna13b_config.json'\n",
    "# model_config = '/home/sunmengjie/lpz/vectordb/ragwatermark/model_configs/vicuna33b_config.json'\n",
    "\n",
    "print(model_config)\n",
    "config = load_json(model_config )\n",
    "llm = Vicuna(config)\n",
    "# nqclean\n",
    "# nfcorpusinject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the model with a prompt\n",
    "query_prompt = \"为什么 from langchain_experimental.graph_transformers import LLMGraphTransformer 提取节点为空\"\n",
    "response = llm.query(query_prompt)\n",
    "\n",
    "# Print the model's response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
